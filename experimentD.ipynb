{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. PERFORMANCE UNDER NEW WORK CONDITIONS\n",
    "In this experiment, we will evaluate effect of the proposed few-shot learning method to address the fourth challenge in limited data fault diagnosis. We mainly pay attention in unbalanced working condition. When new categories appear, we would like to evaluate how well the few-shot performance. We use domain adaptation to simulate new work conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "# set the memory usage\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=tf_config))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import imp\n",
    "import pandas as pd\n",
    "\n",
    "import cwru \n",
    "\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "siamese_net summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2048, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2048, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 100)          51796       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 100)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 51,897\n",
      "Trainable params: 51,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "# imp.reload(models)\n",
    "window_size = 2048\n",
    "\n",
    "siamese_net = models.load_siamese_net((window_size,2))\n",
    "print('\\nsiamese_net summary:')\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sequential_3 is WDCNN:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 128, 16)           2064      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 64, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 64, 32)            1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 64)            4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 6, 64)             12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 3, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               19300     \n",
      "=================================================================\n",
      "Total params: 51,796\n",
      "Trainable params: 51,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('\\nsequential_3 is WDCNN:')\n",
    "siamese_net.layers[2].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52806\n",
      "\n",
      "wdcnn_net summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 2048, 2)           0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 100)               51796     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 52,806\n",
      "Trainable params: 52,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wdcnn_net = models.load_wdcnn_net()\n",
    "print('\\nwdcnn_net summary:')\n",
    "wdcnn_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "import siamese\n",
    "# imp.reload(siamese)\n",
    "import utils\n",
    "# imp.reload(utils)\n",
    "\n",
    "settings = {\n",
    "  \"N_way\": 10,           # how many classes for testing one-shot tasks>\n",
    "  \"batch_size\": 32,\n",
    "  \"best\": -1,\n",
    "  \"evaluate_every\": 600,   # interval for evaluating on one-shot tasks\n",
    "  \"loss_every\": 20,      # interval for printing loss (iterations)\n",
    "  \"n_iter\": 15000,\n",
    "  \"n_val\": 2,          #how many one-shot tasks to validate on?\n",
    "  \"n\": 0,\n",
    "  \"save_path\":\"\",\n",
    "  \"save_weights_file\": \"weights-best-10-oneshot-low-data.hdf5\"\n",
    "}\n",
    "loads ={'A':'1772', 'B':'1750', 'C':'1730'}\n",
    "\n",
    "times = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def WC_train_and_test(exp_name,exps,hps_transfer,is_training):\n",
    "    window_size = 2048\n",
    "    for (hp_from,hp_to) in hps_transfer:\n",
    "        data = cwru.CWRU(['12DriveEndFault'], [loads[i] for i in hp_from],window_size)\n",
    "        print(len(data.X_train),len(data.X_test))\n",
    "        data_test = cwru.CWRU(['12DriveEndFault'], [loads[i] for i in hp_to],window_size)\n",
    "        print(len(data_test.X_train),len(data_test.X_test))\n",
    "        data.X_test, data.y_test = data_test.X_test, data_test.y_test\n",
    "\n",
    "        scores_hps = []\n",
    "        train_classes = sorted(list(set(data.y_train)))\n",
    "        train_indices = [np.where(data.y_train == i)[0] for i in train_classes]\n",
    "        for exp in exps:\n",
    "            # enable the random seed for experimental reproduction\n",
    "            np.random.seed(exp)\n",
    "            scores_1_shot = []\n",
    "            scores_5_shot = []\n",
    "            scores_wdcnn = []\n",
    "            num = int(exp/len(train_classes))\n",
    "            settings['evaluate_every'] = 300 if exp<1000 else 600\n",
    "            print(settings['evaluate_every'])\n",
    "            for time_idx in range(times):\n",
    "                np.random.seed(0)\n",
    "                print(\"\\n%s-%s\"%(exp,time_idx) + '*'*80)\n",
    "                settings[\"save_path\"] = \"tmp/%s/size_%s/%s2%s/time_%s/\" % (exp_name,exp,''.join(hp_from),\n",
    "                                                                           '_'.join(hp_to),time_idx)\n",
    "                data._mkdir(settings[\"save_path\"])\n",
    "\n",
    "                train_idxs = []\n",
    "                val_idxs = []\n",
    "                for i, c in enumerate(train_classes):\n",
    "                    select_idx = train_indices[i][np.random.choice(len(train_indices[i]), num, replace=False)]\n",
    "                    split = int(0.6*num)\n",
    "                    train_idxs.extend(select_idx[:split])\n",
    "                    end = num\n",
    "                    if(0.4*num>100):\n",
    "                        end = split+100\n",
    "                    val_idxs.extend(select_idx[split:end])\n",
    "                X_train, y_train = data.X_train[train_idxs],data.y_train[train_idxs], \n",
    "                X_val, y_val = data.X_train[val_idxs],data.y_train[val_idxs], \n",
    "\n",
    "                print(train_idxs[0:10])\n",
    "                print(val_idxs[0:10])\n",
    "            \n",
    "                # load one-shot model and training\n",
    "                siamese_net = models.load_siamese_net()\n",
    "                siamese_loader = siamese.Siamese_Loader(X_train,\n",
    "                                                    y_train,\n",
    "                                                    X_val,\n",
    "                                                    y_val)\n",
    "                if(is_training):\n",
    "                    print(siamese.train_and_test_oneshot(settings,siamese_net,siamese_loader))\n",
    "\n",
    "                # test 1_shot and 5_shot\n",
    "                print(\"load best weights\",settings[\"save_path\"] + settings['save_weights_file'])\n",
    "                siamese_net.load_weights(settings[\"save_path\"] + settings['save_weights_file'])\n",
    "                siamese_loader.set_val(data.X_test,data.y_test)\n",
    "                s = 'val'\n",
    "                preds_5_shot = []\n",
    "                scores = []\n",
    "                for k in range(5):\n",
    "                    val_acc,preds = siamese_loader.test_oneshot2(siamese_net,len(siamese_loader.classes[s]),\n",
    "                                                                 len(siamese_loader.data[s]),verbose=False)\n",
    "        #             confusion_plot(preds[:,1],preds[:,0])\n",
    "                    print(val_acc)\n",
    "                    scores.append(val_acc)\n",
    "                    preds_5_shot.append(preds[:,1])\n",
    "                preds = []\n",
    "                for line in np.array(preds_5_shot).T:\n",
    "                    preds.append(np.argmax(np.bincount(line)))\n",
    "        #         confusion_plot(np.array(preds),data.y_test) \n",
    "                score_5_shot = accuracy_score(data.y_test,np.array(preds))*100\n",
    "                print('5_shot:',score_5_shot)\n",
    "                scores_1_shot.append(scores[0])\n",
    "                scores_5_shot.append(score_5_shot)\n",
    "\n",
    "\n",
    "                # load wdcnn model and training\n",
    "                y_train = keras.utils.to_categorical(y_train, data.nclasses)\n",
    "                y_val = keras.utils.to_categorical(y_val, data.nclasses)\n",
    "                y_test = keras.utils.to_categorical(data.y_test, data.nclasses)\n",
    "\n",
    "                earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='min')\n",
    "                # checkpoint\n",
    "                filepath=\"%s/weights-best-10-cnn-low-data.hdf5\" % (settings[\"save_path\"])\n",
    "                checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "                tb = keras.callbacks.TensorBoard(log_dir='./tmp/logs', histogram_freq=4, \n",
    "                                                 batch_size=32, write_graph=True, \n",
    "                                                 write_grads=False, write_images=True, \n",
    "                                                 embeddings_freq=0, embeddings_layer_names=None, \n",
    "                                                 embeddings_metadata=None, embeddings_data=None, \n",
    "                                                 update_freq='epoch')\n",
    "\n",
    "                # callbacks_list = [earlyStopping,checkpoint,tb]\n",
    "                callbacks_list = [earlyStopping,checkpoint]\n",
    "\n",
    "                wdcnn_net = models.load_wdcnn_net()\n",
    "\n",
    "                if(is_training):\n",
    "                    wdcnn_net.fit(X_train, y_train,\n",
    "                              batch_size=32,\n",
    "                              epochs=300,\n",
    "                              verbose=0,\n",
    "                              callbacks=callbacks_list,\n",
    "                              validation_data=(X_val, y_val))\n",
    "\n",
    "                # test wdcnn\n",
    "                wdcnn_net.load_weights(filepath)\n",
    "                score = wdcnn_net.evaluate(data.X_test, y_test, verbose=0)\n",
    "                print('Test loss:', score[0])\n",
    "                print('Test accuracy:', score[1])\n",
    "                scores_wdcnn.append(score[1]*100)\n",
    "\n",
    "\n",
    "            a =pd.DataFrame(np.array(scores_1_shot))\n",
    "    #         a.columns = str(exp)\n",
    "            a.to_csv(\"tmp/%s/size_%s/%s2%s/scores_1_shot.csv\" % (exp_name,exp,''.join(hp_from),'_'.join(hp_to)),index=True)\n",
    "\n",
    "\n",
    "            a =pd.DataFrame(np.array(scores_5_shot))\n",
    "    #         a.columns = str(exp)\n",
    "            a.to_csv(\"tmp/%s/size_%s/%s2%s/scores_5_shot.csv\" % (exp_name,exp,''.join(hp_from),'_'.join(hp_to)),index=True)\n",
    "\n",
    "\n",
    "            a =pd.DataFrame(np.array(scores_wdcnn))\n",
    "    #         a.columns = str(exp)\n",
    "            a.to_csv(\"tmp/%s/size_%s/%s2%s/scores_wdcnn.csv\" % (exp_name,exp,''.join(hp_from),'_'.join(hp_to)),index=True)  \n",
    "\n",
    "        \n",
    "def WC_analysis(exp_name,exps,hps_transfer):\n",
    "    scores_1_shot_all = pd.DataFrame()\n",
    "    scores_5_shot_all = pd.DataFrame()\n",
    "    scores_wdcnn_all = pd.DataFrame()\n",
    "    for (hp_from,hp_to) in hps_transfer:\n",
    "        for exp in exps:\n",
    "            hps = \"%s2%s\" % (''.join(hp_from),'_'.join(hp_to))\n",
    "            file_path = \"tmp/%s/size_%s/%s\" % (exp_name,exp,hps)\n",
    "            tmp_data = pd.read_csv(\"%s/scores_1_shot.csv\" % (file_path), \n",
    "                                   sep=',', index_col=0)\n",
    "            tmp_data['hps'] = hps\n",
    "            tmp_data['exp'] = exp \n",
    "            scores_1_shot_all = pd.concat([scores_1_shot_all,tmp_data],axis=0)\n",
    "\n",
    "            tmp_data = pd.read_csv(\"%s/scores_5_shot.csv\" % (file_path), \n",
    "                                   sep=',', index_col=0)\n",
    "            tmp_data['hps'] = hps\n",
    "            tmp_data['exp'] = exp \n",
    "            scores_5_shot_all = pd.concat([scores_5_shot_all,tmp_data],axis=0)\n",
    "\n",
    "            tmp_data = pd.read_csv(\"%s/scores_wdcnn.csv\" % (file_path), \n",
    "                                   sep=',', index_col=0)\n",
    "            tmp_data['hps'] = hps\n",
    "            tmp_data['exp'] = exp \n",
    "            scores_wdcnn_all = pd.concat([scores_wdcnn_all,tmp_data],axis=0)\n",
    "\n",
    "\n",
    "    scores_1_shot_all.to_csv(\"tmp/%s/scores_1_shot_all.csv\" % (exp_name), float_format='%.6f', index=True)\n",
    "    scores_5_shot_all.to_csv(\"tmp/%s/scores_5_shot_all.csv\" % (exp_name), float_format='%.6f', index=True)\n",
    "    scores_wdcnn_all.to_csv(\"tmp/%s/scores_wdcnn_all.csv\" % (exp_name), float_format='%.6f', index=True)\n",
    "\n",
    "    scores_1_shot_all['model'] = 'One-shot'\n",
    "    scores_5_shot_all['model'] = 'Five-shot'\n",
    "    scores_wdcnn_all['model'] = 'WDCNN'\n",
    "\n",
    "    scores_all = pd.concat([scores_1_shot_all,scores_5_shot_all,scores_wdcnn_all],axis=0)\n",
    "    scores_all.to_csv(\"tmp/%s/scores_all.csv\" % (exp_name), float_format='%.6f', index=True)   \n",
    "    \n",
    "    return scores_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario setting for domain adaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"EXP-D\"\n",
    "exps = [90,6600]\n",
    "hps_transfer = [\n",
    "    (['A'],['B']),\n",
    "    (['A'],['C']),\n",
    "    (['B'],['A']),\n",
    "    (['B'],['C']),\n",
    "    (['C'],['A']),\n",
    "    (['C'],['B']),\n",
    "]\n",
    "\n",
    "is_training = False    # enable or disable train models. if enable training, save best models will be update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WC_train_and_test(exp_name,exps,hps_transfer,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                    0                                   \n",
       " hps               A2B    A2C    B2A    B2C    C2A    C2B\n",
       " exp  model                                              \n",
       " 90   WDCNN      76.96  77.92  75.00  78.24  75.00  80.68\n",
       "      One-shot   84.24  79.80  79.24  77.28  71.16  74.00\n",
       "      Five-shot  86.60  82.08  80.84  79.04  71.96  74.96\n",
       " 6600 WDCNN      97.08  91.48  93.00  91.80  78.84  85.88\n",
       "      One-shot   98.92  90.44  85.36  90.12  60.36  65.36\n",
       "      Five-shot  99.24  90.40  85.28  90.32  60.52  65.36,\n",
       "                        0                                                    \n",
       " hps                  A2B        A2C       B2A        B2C       C2A       C2B\n",
       " exp  model                                                                  \n",
       " 90   WDCNN      8.099822   8.021748  9.618963   6.767767  5.936703  5.376657\n",
       "      One-shot   5.287133   6.069962  5.186135  10.377411  4.951139  8.504117\n",
       "      Five-shot  5.443038   5.926363  4.531176  10.842222  4.827514  9.373627\n",
       " 6600 WDCNN      2.004883  12.338990  3.289039   2.474761  2.978143  9.187528\n",
       "      One-shot   1.687075   6.500462  7.507507  11.282317  3.966303  6.864271\n",
       "      Five-shot  1.040513   6.480055  7.551424  11.518951  3.704892  6.825312)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_all = WC_analysis(exp_name,exps,hps_transfer)\n",
    "scores_all_mean = scores_all.groupby(['exp','model','hps']).mean().unstack()\\\n",
    "                .sort_values(by=['model'],ascending=0).sort_values(by=['exp'],ascending=1)\n",
    "scores_all_std = scores_all.groupby(['exp','model','hps']).std().unstack()\\\n",
    "                .sort_values(by=['model'],ascending=0).sort_values(by=['exp'],ascending=1)\n",
    "scores_all_mean.to_csv(\"tmp/%s/scores_all_mean.csv\" % (exp_name), float_format='%.2f', index=True)\n",
    "scores_all_std.to_csv(\"tmp/%s/scores_all_std.csv\" % (exp_name), float_format='%.2f', index=True)\n",
    "scores_all_mean, scores_all_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario setting for A2C and C2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"EXP-D\"\n",
    "# 90 and 6600 exp have trained done before\n",
    "exps = [120,300,900,1500,3000]\n",
    "hps_transfer = [\n",
    "#     (['A'],['B']),\n",
    "    (['A'],['C']),\n",
    "#     (['B'],['A']),\n",
    "#     (['B'],['C']),\n",
    "    (['C'],['A']),\n",
    "#     (['C'],['B']),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC_train_and_test(exp_name,exps,hps_transfer,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(exp               90     120    300    900    1500   3000   6600\n",
       "   hps model                                                     \n",
       " 0 A2C WDCNN      77.92  81.00  94.12  93.52  96.44  94.04  91.48\n",
       "       One-shot   79.80  86.84  85.80  91.60  92.36  90.96  90.44\n",
       "       Five-shot  82.08  88.48  86.92  92.52  92.40  91.00  90.40\n",
       "   C2A WDCNN      75.00  73.52  78.00  73.72  78.92  75.16  78.84\n",
       "       One-shot   71.16  70.40  67.12  63.68  62.00  60.64  60.36\n",
       "       Five-shot  71.96  70.64  67.20  63.68  62.28  60.56  60.52,\n",
       " exp                  90        120       300       900       1500      3000  \\\n",
       "   hps model                                                                   \n",
       " 0 A2C WDCNN      8.021748  7.669275  2.945731  3.727615  1.773384  5.601428   \n",
       "       One-shot   6.069962  5.502161  7.962691  3.567757  3.509733  6.972199   \n",
       "       Five-shot  5.926363  5.935355  8.394813  3.155524  3.868965  7.352097   \n",
       "   C2A WDCNN      5.936703  4.360632  5.222175  4.891898  5.891010  4.820143   \n",
       "       One-shot   4.951139  3.636848  4.750860  5.398930  4.353287  5.140514   \n",
       "       Five-shot  4.827514  3.508466  5.222175  5.771347  4.279356  5.320652   \n",
       " \n",
       " exp                   6600  \n",
       "   hps model                 \n",
       " 0 A2C WDCNN      12.338990  \n",
       "       One-shot    6.500462  \n",
       "       Five-shot   6.480055  \n",
       "   C2A WDCNN       2.978143  \n",
       "       One-shot    3.966303  \n",
       "       Five-shot   3.704892  )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysis\n",
    "exps = [90,120,300,900,1500,3000,6600]\n",
    "scores_all = WC_analysis(exp_name,exps,hps_transfer)\n",
    "scores_all_mean = scores_all.groupby(['exp','model','hps']).mean().unstack().unstack().T \\\n",
    "            .sort_values(by=['model'],ascending=0).sort_values(by=['hps'],ascending=1)\n",
    "scores_all_std = scores_all.groupby(['exp','model','hps']).std().unstack().unstack().T \\\n",
    "            .sort_values(by=['model'],ascending=0).sort_values(by=['hps'],ascending=1)\n",
    "scores_all_mean.to_csv(\"tmp/%s/scores_AC_mean.csv\" % (exp_name), float_format='%.2f', index=True)\n",
    "scores_all_std.to_csv(\"tmp/%s/scores_AC_std.csv\" % (exp_name), float_format='%.2f', index=True)\n",
    "scores_all_mean, scores_all_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New scenario setting for domain adaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"EXP-D-NEW\"\n",
    "exps = [90,13200]\n",
    "hps_transfer = [\n",
    "    (['A','B'],['C']),\n",
    "    (['A','C'],['B']),\n",
    "    (['B','C'],['A']),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC_train_and_test(exp_name,exps,hps_transfer,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                     0              \n",
       " hps               AB2C   AC2B   BC2A\n",
       " exp   model                         \n",
       " 90    WDCNN      73.32  80.72  75.24\n",
       "       One-shot   83.72  87.76  81.24\n",
       "       Five-shot  85.12  90.56  83.24\n",
       " 13200 WDCNN      96.04  99.28  94.04\n",
       "       One-shot   96.84  99.80  93.96\n",
       "       Five-shot  97.00  99.84  94.32,\n",
       "                         0                    \n",
       " hps                  AB2C      AC2B      BC2A\n",
       " exp   model                                  \n",
       " 90    WDCNN      9.339141  8.112103  6.771049\n",
       "       One-shot   7.339513  4.561481  3.673085\n",
       "       Five-shot  6.326804  3.642710  4.154569\n",
       " 13200 WDCNN      4.507574  1.395867  3.479208\n",
       "       One-shot   3.970782  0.388730  5.148290\n",
       "       Five-shot  3.906689  0.386437  4.743370)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_all = WC_analysis(exp_name,exps,hps_transfer)\n",
    "scores_all_mean = scores_all.groupby(['exp','model','hps']).mean().unstack()\\\n",
    "                .sort_values(by=['model'],ascending=0).sort_values(by=['exp'],ascending=1)\n",
    "scores_all_std = scores_all.groupby(['exp','model','hps']).std().unstack()\\\n",
    "                .sort_values(by=['model'],ascending=0).sort_values(by=['exp'],ascending=1)\n",
    "scores_all_mean.to_csv(\"tmp/%s/scores_all_mean.csv\" % (exp_name), float_format='%.2f', index=True)\n",
    "scores_all_std.to_csv(\"tmp/%s/scores_all_std.csv\" % (exp_name), float_format='%.2f', index=True)\n",
    "scores_all_mean, scores_all_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
